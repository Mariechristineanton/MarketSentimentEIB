{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#This project was presented at the headquarters of the EIB in Luxembourg. I will upload the .ipynb document with the database becase it was running for hours.\n\n#1.\tScrape data → build daily aggregated sentiment with VADER\n#2.\tDo the same for historical news\n#3.\tMerge or concatenate them\n#4.\tMatch with S&P 500 daily data\n#5.\tExport a final “master” CSV that has sentiment and the daily S&P 500 variation\n\n#Download Reuters news from the archive (all news from 2020 until circa 2022 (when project was done))\n\nimport requests\nimport gettext\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n#Defining some variables to download data\n\nurl_list = [\"https://www.reuters.com/news/archive?view=page&page=9\"]\nurl = \"https://www.reuters.com/news/archive?view=page&page=\"\ntitle = []\ndate =[]\nsumm = []\ndicc = pd.DataFrame(columns = [\"title\",\"date\", \"summary\"])\nfinal_dicc = pd.DataFrame()\nremove = '</h3>, <h3 class=\"story-title\">'\ni=0\n\n\n#Code for data download using beautiful soup to find the title, summary and date\npage = range(10,3000)\nfor num in page:\nurl_2 = url+str(num)+\"&pageSize=10\"\nurl_list.append(url_2)\n#print(url_list)\n\nfor link in url_list:\ni+=1\npage = requests.get(link)\nsoup = BeautifulSoup(page.text,'html.parser')\nresults = soup.find_all(\"h3\", class_=\"story-title\")\nresults.pop(-1)\nresults.pop(-1)\nresults.pop(-1)\nfor result in results:\ntitle.append(result.string)\n#print(title)\n\ndate_art = soup.find_all(\"span\", class_=\"timestamp\")\nfor dates in date_art:\ndate.append(dates.string)\n#print(date)\n\nsummary = soup.find_all(\"p\")\nsummary.pop(-1)\nsummary.pop(-1)\nsummary.pop(-1)\nsummary.pop(-1)\nfor summa in summary:\nsumm.append(summa.string)\n#print(summ)\n\n\n\ndicc[\"title\"]= title\ndicc[\"date\"]= date\ndicc[\"summary\"]=summ\nprint(dicc)\n\n#Save file into a CSV document\n\ndicc.to_csv('news_reuters.csv')\n\n#Create sentiment by day\n\n\nimport pandas as pd\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n#Create lists\nvader_dict={}\ntitles_negative_score=0\ntitles_positive_score=0\ntitles_neutral_score=0\ntitles_compound_score=0\nsia = SentimentIntensityAnalyzer()\nsentiment_dicc={}\nfilenamel=[]\ndatel=[]\npositivel=[]\nnegativel=[]\nneutrall=[]\ncompoundl=[]\nsentiment_dicc={\"Date\":[], \"Positive\":[], \"Negative\":[], \"Neutral\":[], \"Compound\":[]}\nfinalsdf=pd.DataFrame()\nfinalvader_list=pd.DataFrame()\nnews_by_date={}\n\ntitles_negative_score=0\ntitles_positive_score_=0\ntitles_neutral_score=0\ntitles_compound_score=0\n\n\ndata=pd.read_csv('news_reuters.csv', header=0)\n\n\nfor i in range(1, data.shape[0]):\n\ndate = data[\"date\"][i]\n#news_by_date_hist[date_hist] = []\n\nvader_list=[]\ntitle = data[\"title\"][i]\n\n#print(title_hist)\n\ntitle_score=sia.polarity_scores(title)\n#print(title_score_hist)\n\ntitles_negative_score=title_score[\"neg\"]\ntitles_positive_score=title_score[\"pos\"]\ntitles_neutral_score=title_score[\"neu\"]\ntitles_compound_score=title_score[\"compound\"]\n\n\nvader_list.append(title_score)\nvader_dict[i]= vader_list\n\n\ndatel.append(date)\n\npositivel.append(titles_positive_score)\nnegativel.append(titles_negative_score)\nneutrall.append(titles_neutral_score)\ncompoundl.append(titles_compound_score)\n\nsentiment_dicc[\"Date\"]=datel\nsentiment_dicc[\"Positive\"]=positivel\nsentiment_dicc[\"Negative\"]=negativel\nsentiment_dicc[\"Neutral\"]=neutrall\nsentiment_dicc[\"Compound\"]=compoundl\n\n\n\n#print(sentiment_dicc)\n\nsdf = pd.DataFrame(sentiment_dicc, columns=['Date', 'Positive', 'Negative', 'Neutral', 'Compound' ])\n\nfinalsdf = finalsdf.append(sdf, ignore_index = True)\n\nprint(sdf)\n\n\n#Save a file with the sentiment scores of all the news.\n\nfinalsdf.to_csv(\"Sentiment_scores_Reuters.csv\")\n\n\n\n# Group by day each news\n\ndata=pd.read_csv(\"Sentiment_scores_Reuters.csv\")\n\ndf_scoreday_r = data.groupby(['Date']).sum()\n\n\n#Save a file with the scores\n\ndf_scoreday_r.to_csv(\"Score_day_Reuters.csv\")\n\nprint(df_scoreday_r)\n\n\n#___________\n\n#Sentiment historic data\n\n#The document us_equities_news_dataset.csv should be downloaded from:\n# https://www.kaggle.com/datasets/gennadiyr/us-equities-news-data or https://drive.google.com/file/d/1BstjOHk_6YDWWmIBGhORMUORGqQM9tYV/view?usp=sharing .\n\n\nimport pandas as pd\nimport nltk\nimport pandas as pd\nnltk.download('vader_lexicon')\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n\n#Create lists\nvader_dict_hist={}\ntitles_negative_score_hist=0\ntitles_positive_score_hist=0\ntitles_neutral_score_hist=0\ntitles_compound_score_hist=0\nsia_hist = SentimentIntensityAnalyzer()\nsentiment_dicc_hist={}\nfilenamel_hist=[]\ndatel_hist=[]\npositivel_hist=[]\nnegativel_hist=[]\nneutrall_hist=[]\ncompoundl_hist=[]\nsentiment_dicc_hist={\"Date\":[], \"Positive\":[], \"Negative\":[], \"Neutral\":[], \"Compound\":[]}\nfinalsdf_hist=pd.DataFrame()\nfinalvader_list_hist=pd.DataFrame()\nnews_by_date_hist={}\n\ntitles_negative_score_hist=0\ntitles_positive_score_hist=0\ntitles_neutral_score_hist=0\ntitles_compound_score_hist=0\n\nsia = SentimentIntensityAnalyzer()\n\n#data_hist=pd.read_csv(\"trial_us.csv\", header=0)\ndata_hist=pd.read_csv(\"us_equities_news_dataset.csv\", header=0)\n\n#print(data_hist)\n\n#Identify each new and create a sentiment for each specific day\n\nfor i in range(1, data_hist.shape[0]):\n\ndate_hist = data_hist[\"release_date\"][i]\n#news_by_date_hist[date_hist] = []\n\nvader_list_hist=[]\ntitle_hist = data_hist[\"title\"][i]\n\n#print(title_hist)\n\ntitle_score_hist=sia.polarity_scores(title_hist)\n#print(title_score_hist)\n\ntitles_negative_score_hist=title_score_hist[\"neg\"]\ntitles_positive_score_hist=title_score_hist[\"pos\"]\ntitles_neutral_score_hist=title_score_hist[\"neu\"]\ntitles_compound_score_hist=title_score_hist[\"compound\"]\n\nvader_list_hist.append(title_score_hist)\nvader_dict_hist[i]= vader_list_hist\n\nfilenamel_hist.append(title_hist)\n#print(filenamel_hist)\n\n\ndatel_hist.append(date_hist)\npositivel_hist.append(titles_positive_score_hist)\nnegativel_hist.append(titles_negative_score_hist)\nneutrall_hist.append(titles_neutral_score_hist)\ncompoundl_hist.append(titles_compound_score_hist)\n\nsentiment_dicc_hist[\"Date\"]=datel_hist\nsentiment_dicc_hist[\"Positive\"]=positivel_hist\nsentiment_dicc_hist[\"Negative\"]=negativel_hist\nsentiment_dicc_hist[\"Neutral\"]=neutrall_hist\nsentiment_dicc_hist[\"Compound\"]=compoundl_hist\n\n\n#sdf_hist = pd.DataFrame(sentiment_dicc_hist, columns=['Date', 'Positive', 'Negative', 'Neutral', 'Compound' ])\n\n#for day in news_by_date_hist:\n# if day == date_hist:\n\n# news_by_date[day].append(\n\n#print(sentiment_dicc_hist)\n\nsdf_hist = pd.DataFrame(sentiment_dicc_hist, columns=['Date', 'Positive', 'Negative', 'Neutral', 'Compound' ])\n\nfinalsdf_hist = finalsdf_hist.append(sdf_hist, ignore_index = True)\n\nprint(sdf_hist)\n\n#Save a file with the sentiment scores of all the news.\n\nfinalsdf_hist.to_csv(\"Sentiment_scores_hist.csv\")\n\n\n\n# Group by day each news\n\ndata_hist=pd.read_csv(\"Sentiment_scores_hist.csv\")\n\ndf_scoreday = data_hist.groupby(['Date']).sum()\n\n#Save a file with the scores\n\ndf_scoreday.to_csv(\"Score_day_historic.csv\")\n\nprint(df_scoreday)\n\n# ____________\n\n\n# Plot of S&P500\n#Requires to download and put in the same folder as the python file the document \"S&P500(historic).xlsx\"\n#It can be downloaded from: https://docs.google.com/spreadsheets/d/1A_zI2OIDMB6zZekjg3TEcvVnLCumNv7x/edit?usp=sharing&ouid=108237498879566772117&rtpof=true&sd=true\n\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\ndatastockh = pd.read_excel(\"S&P500(historic).xlsx\")\n\nclose = datastockh.Cierre\ndate = datastockh.Date\n\n\n\nplt.plot (date, close)\nplt.show()\n\n\n\n# Plot data sentiment VS S&P500\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\n\nscoreday = pd.read_csv(\"Score_day_Reuters.csv\", header=0)\nscoreday_hist = pd.read_csv(\"Score_day_historic.csv\", header=0) #TRIALLL\n\n\n\n#Sum of day sentiment\ndflist = [\"Positive\", \"Negative\", \"Neutral\"]\nposneg = [\"Positive\", \"Negative\"]\n\n\nscoreday[\"news_day\"]=scoreday[dflist].sum(axis=1)\n\nscoreday[\"Positive_avg\"]=scoreday[\"Positive\"]/scoreday[\"news_day\"]\nscoreday[\"Negative_avg\"]=scoreday[\"Negative\"]/scoreday[\"news_day\"]\nscoreday[\"Neutral_avg\"]=scoreday[\"Neutral\"]/scoreday[\"news_day\"]\nscoreday.to_csv(\"Score_day_Reuters_sum.csv\")\n\nprint(scoreday)\n\npositive_avg=scoreday[\"Positive_avg\"]\nnegative_avg=scoreday[\"Negative_avg\"]\nneutral_avg=scoreday[\"Neutral_avg\"]\n\n\n#######\n\n\npositive = scoreday[\"Positive\"]\nnegative = scoreday[\"Negative\"]\n\n\npositive_hist = scoreday_hist[\"Positive\"]\nnegative_hist = scoreday_hist[\"Negative\"]\nneutral_history = scoreday_hist[\"Neutral\"]\ndate_hist = scoreday_hist[\"Date\"]\n\nscoreday_hist[\"news_day\"]=scoreday_hist[dflist].sum(axis=1)\nscoreday_hist[\"Positive_avg\"]=scoreday_hist[\"Positive\"]/scoreday_hist[\"news_day\"]\nscoreday_hist[\"Negative_avg\"]=scoreday_hist[\"Negative\"]/scoreday_hist[\"news_day\"]\nscoreday_hist[\"Neutral_avg\"]=scoreday_hist[\"Neutral\"]/scoreday_hist[\"news_day\"]\nscoreday_hist.to_csv(\"Score_day_historic_sum.csv\")\n\npositive_hist_avg=scoreday_hist[\"Positive_avg\"]\nnegative_hist_avg=scoreday_hist[\"Negative_avg\"]\nneutral_hist_avg=scoreday_hist[\"Neutral_avg\"]\n\nprint(scoreday_hist)\n\n#plt.plot(date_hist, positive_hist_avg*10)\n#plt.plot(date_hist, negative_hist_avg*10)\n\n#plt.show()\n\n\n\n\n#Create a Sentiment for the last imported news (reuters)\n\nimport nltk\nimport pandas as pd\nnltk.download('vader_lexicon')\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n\n\nfilename = 'news_reuters.csv'\ndata = pd.read_csv(filename, header=0)\n\nvader_dict={}\ntitles_negative_score=0\ntitles_positive_score=0\ntitles_neutral_score=0\ntitles_compound_score=0\n\n\nsia = SentimentIntensityAnalyzer()\n\n\nfor i in range(1, data.shape[0]):\nvader_list=[]\ntitle = data[\"title\"][i]\n\ntitle_score=sia.polarity_scores(title)\n\ntitles_negative_score+=title_score[\"neg\"]\ntitles_positive_score+=title_score[\"pos\"]\ntitles_neutral_score+=title_score[\"neu\"]\ntitles_compound_score+=title_score[\"compound\"]\n\nvader_list.append(title_score)\nvader_dict[i]= vader_list\n\n\n#print(vader_dict)\nprint(\"Negative sum is: \", titles_negative_score)\nprint(\"Neutral sum is: \", titles_neutral_score)\nprint(\"Positive sum is: \", titles_positive_score)\ntotal_score= titles_negative_score + titles_neutral_score + titles_positive_score\nprint(\"Total sum is: \", total_score)\nprint(\"Compound sum is: \",titles_compound_score)\n\n\n#Create a Pie Chart\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nscores = [titles_negative_score, titles_neutral_score, titles_positive_score]\n\nlabels = [\"Negative\", \"Neutral\", \"Positive\"]\n\n\nif titles_negative_score >= titles_positive_score:\nexplode = [0.2, 0, 0]\n\nif titles_negative_score <= titles_positive_score:\nexplode = [0, 0, 0.2]\n\nplt.pie(scores, labels=labels, explode=explode, autopct=\"%1.1f%%\", wedgeprops={\"edgecolor\":\"black\"})\n\nplt.title(\"Scores\")\nplt.tight_layout()\nplt.show()\n\n\n#_______________\n\nimport pandas as pd\ndf = pd.read_csv(\"Score_day_Reuters_sum.csv\")\n#ESTA PRIMERA PARTE CAMBIA LAS FECHAS AL FORMATO CORRECTO\ndates = []\n\nfor i in range(0, df.shape[0]):\n#print(df[\"Date\"][i])\ndate = df[\"Date\"][i]\nmonth = date.split()[0]\n#print(date.split())\nif month == \"Jan\":\nmonth = \"01\"\nelif month == \"Feb\":\nmonth = \"02\"\nelif month == \"Mar\":\nmonth = \"03\"\nelif month == \"Apr\":\nmonth = \"04\"\nelif month == \"May\":\nmonth = \"05\"\nelif month == \"Jun\":\nmonth = \"06\"\nelif month == \"Jul\":\nmonth = \"07\"\nelif month == \"Aug\":\nmonth = \"08\"\nelif month == \"Sep\":\nmonth = \"09\"\nelif month == \"Oct\":\nmonth = \"10\"\nelif month == \"Nov\":\nmonth = \"11\"\nelif month == \"Dec\":\nmonth = \"12\"\n\nreal_date= str(date.split()[2]) + \"-\" + str(month) + \"-\" + str(date.split()[1])\ndates.append(real_date)\n\n\ndf[\"Date\"]=dates\n\n\n#ESTA SEGUNDA PARTE CREA LOS NUEVOS INDICADORES\npositive_deviation_sd=[]\nnegative_deviation_sd=[]\nneutral_deviation_sd=[]\n\ndifference_positive_avg_negative_avg=[]\n\n\npositive_avg_mean=df['Positive_avg'].mean()\nnegative_avg_mean=df['Negative_avg'].mean()\nneutral_avg_mean=df['Neutral_avg'].mean()\n\npositive_avg_standard_dev = df[\"Positive_avg\"].std()\nnegative_avg_standard_dev = df[\"Negative_avg\"].std()\nneutral_avg_standard_dev= df[\"Neutral_avg\"].std()\n\nfor i in range(0, df.shape[0]):\npositive_deviation=(df[\"Positive_avg\"][i]-positive_avg_mean)/positive_avg_standard_dev\nnegative_deviation=(df[\"Negative_avg\"][i]-negative_avg_mean)/negative_avg_standard_dev\nneutral_deviation=(df[\"Neutral_avg\"][i]-neutral_avg_mean)/neutral_avg_standard_dev\n\npositive_deviation_sd.append(positive_deviation)\nnegative_deviation_sd.append(negative_deviation)\nneutral_deviation_sd.append(neutral_deviation)\n\ndifference_pos_neg= df[\"Positive_avg\"][i] - df[\"Negative_avg\"][i]\ndifference_positive_avg_negative_avg.append(difference_pos_neg)\n\n\ndf[\"positive_deviation_sd\"]=positive_deviation_sd\ndf[\"negative_deviation_sd\"]=negative_deviation_sd\ndf[\"neutral_deviation_sd\"]=neutral_deviation_sd\ndf[\"difference_positive_negative\"]=difference_positive_avg_negative_avg\n\n\n#GUARDAR LA NUEVA DATABASE EN CSV!\ndf.to_csv(\"score_day_reuters_definitive.csv\")\n\n\n#_____________\n\nimport pandas as pd\ndf = pd.read_csv(\"Score_day_historic_sum.csv\")\n\n#ESTA SEGUNDA PARTE CREA LOS NUEVOS INDICADORES\npositive_deviation_sd=[]\nnegative_deviation_sd=[]\nneutral_deviation_sd=[]\n\ndifference_positive_avg_negative_avg=[]\n\n\npositive_avg_mean=df['Positive_avg'].mean()\nnegative_avg_mean=df['Negative_avg'].mean()\nneutral_avg_mean=df['Neutral_avg'].mean()\n\npositive_avg_standard_dev = df[\"Positive_avg\"].std()\nnegative_avg_standard_dev = df[\"Negative_avg\"].std()\nneutral_avg_standard_dev= df[\"Neutral_avg\"].std()\n\nfor i in range(0, df.shape[0]):\npositive_deviation=(df[\"Positive_avg\"][i]-positive_avg_mean)/positive_avg_standard_dev\nnegative_deviation=(df[\"Negative_avg\"][i]-negative_avg_mean)/negative_avg_standard_dev\nneutral_deviation=(df[\"Neutral_avg\"][i]-neutral_avg_mean)/neutral_avg_standard_dev\n\npositive_deviation_sd.append(positive_deviation)\nnegative_deviation_sd.append(negative_deviation)\nneutral_deviation_sd.append(neutral_deviation)\n\ndifference_pos_neg= df[\"Positive_avg\"][i] - df[\"Negative_avg\"][i]\ndifference_positive_avg_negative_avg.append(difference_pos_neg)\n\n\ndf[\"positive_deviation_sd\"]=positive_deviation_sd\ndf[\"negative_deviation_sd\"]=negative_deviation_sd\ndf[\"neutral_deviation_sd\"]=neutral_deviation_sd\ndf[\"difference_positive_negative\"]=difference_positive_avg_negative_avg\n\n\n#GUARDAR LA NUEVA DATABASE EN CSV!\ndf.to_csv(\"score_day_historic_definitive.csv\")\n\n#_____________\n\nimport pandas as pd\n\nSP_change=[]\n\n#import reuters and historic docuemts to join them in one data base\ndf_1 = pd.read_csv(\"score_day_reuters_definitive.csv\")\ndf_2 = pd.read_csv(\"score_day_historic_definitive.csv\")\n\n#import the document with the historic data of the S&P 500\ndf_4= pd.read_csv(\"Datos_históricos_S&P_500.csv\")\n\n#Create a new database\ndf_3=pd.concat([df_1, df_2], ignore_index=True)\n\n\n#Change to the same date format both documents\ndf_3[\"Date\"] = pd.to_datetime(df_3[\"Date\"])\ndf_4[\"Date\"] = pd.to_datetime(df_4[\"Date\"])\n\n\n\n#Find the varation of the S&P500 of the specific date that we have on our database\n\nfor i in df_3[\"Date\"]:\nfor j in df_4[\"Date\"]:\nif i == j:\nprint(i)\n\nvar = df_4.loc[df_4[\"Date\"]==j, \"Var\"].iloc[0]\n#print(var)\n\ndf_3.loc[df_3[\"Date\"]==j, [\"S&P_Change\"]]= var\n#print(df_3)\n\n\n\ndf_3.to_csv(\"database_s&p.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
